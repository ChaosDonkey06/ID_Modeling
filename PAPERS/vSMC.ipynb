{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.extend import notrace_primitive\n",
    "\n",
    "@notrace_primitive\n",
    "def resampling(w, rs):\n",
    "    \"\"\"\n",
    "    Stratified resampling with \"nograd_primitive\" to ensure autograd\n",
    "    takes no derivatives through it.\n",
    "    \"\"\"\n",
    "    N = w.shape[0]\n",
    "    bins = np.cumsum(w)\n",
    "    ind = np.arange(N)\n",
    "    u = (ind  + rs.rand(N))/N\n",
    "\n",
    "    return np.digitize(u, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsmc_lower_bound(prop_params, model_params, y, smc_obj, rs, verbose=False, adapt_resamp=False):\n",
    "    \"\"\"\n",
    "    Estimate the VSMC lower bound. Amenable to (biased) reparameterization\n",
    "    gradients\n",
    "    .. math::\n",
    "        ELBO(\\theta,\\lambda) =\n",
    "        \\mathbb{E}_{\\phi}\\left[\\nabla_\\lambda \\log \\hat p(y_{1:T}) \\right]\n",
    "    Requires an SMC object with 2 member functions:\n",
    "    -- sim_prop(t, x_{t-1}, y, prop_params, model_params, rs)\n",
    "    -- log_weights(t, x_t, x_{t-1}, y, prop_params, model_params)\n",
    "    \"\"\"\n",
    "    # Extract constants\n",
    "    T = y.shape[0]\n",
    "    Dx = smc_obj.Dx\n",
    "    N = smc_obj.N\n",
    "\n",
    "    # Initialize SMC\n",
    "    X = np.zeros((N,Dx))\n",
    "    Xp = np.zeros((N,Dx))\n",
    "    logW = np.zeros(N)\n",
    "    W = np.exp(logW)\n",
    "    W /= np.sum(W)\n",
    "    logZ = 0.\n",
    "    ESS = 1./np.sum(W**2)/N\n",
    "\n",
    "    for t in range(T):\n",
    "        # Resampling\n",
    "        if adapt_resamp:\n",
    "            if ESS < 0.5:\n",
    "                ancestors = resampling(W, rs)\n",
    "                Xp = X[ancestors]\n",
    "                logZ = logZ + max_logW + np.log(np.sum(W)) - np.log(N)\n",
    "                logW = np.zeros(N)\n",
    "            else:\n",
    "                Xp = X\n",
    "        else:\n",
    "            if t > 0:\n",
    "                ancestors = resampling(W, rs)\n",
    "                Xp = X[ancestors]\n",
    "            else:\n",
    "                Xp = X\n",
    "\n",
    "        # Propagation\n",
    "        X = smc_obj.sim_prop(t, Xp, y, prop_params, model_params, rs)\n",
    "\n",
    "        # Weighting\n",
    "        if adapt_resamp:\n",
    "            logW = logW + smc_obj.log_weights(t, X, Xp, y, prop_params, model_params)\n",
    "        else:\n",
    "            logW = smc_obj.log_weights(t, X, Xp, y, prop_params, model_params)\n",
    "        max_logW = np.max(logW)\n",
    "        W = np.exp(logW-max_logW)\n",
    "        if adapt_resamp:\n",
    "            if t == T-1:\n",
    "                logZ = logZ + max_logW + np.log(np.sum(W)) - np.log(N)\n",
    "        else:\n",
    "            logZ = logZ + max_logW + np.log(np.sum(W)) - np.log(N)\n",
    "        W /= np.sum(W)\n",
    "        ESS = 1./np.sum(W**2)/N\n",
    "    if verbose:\n",
    "        print('ESS: '+str(ESS))\n",
    "    return logZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_q(prop_params, model_params, y, smc_obj, rs, verbose=False):\n",
    "    \"\"\"\n",
    "    Simulates a single sample from the VSMC approximation.\n",
    "    Requires an SMC object with 2 member functions:\n",
    "    -- sim_prop(t, x_{t-1}, y, prop_params, model_params, rs)\n",
    "    -- log_weights(t, x_t, x_{t-1}, y, prop_params, model_params)\n",
    "    \"\"\"\n",
    "    # Extract constants\n",
    "    T  = y.shape[0]\n",
    "    Dx = smc_obj.Dx\n",
    "    N  = smc_obj.N\n",
    "\n",
    "    # Initialize SMC\n",
    "    X = np.zeros((N,T,Dx))\n",
    "    logW = np.zeros(N)\n",
    "    W = np.zeros((N,T))\n",
    "    ESS = np.zeros(T)\n",
    "\n",
    "    for t in range(T):\n",
    "        # Resampling\n",
    "        if t > 0:\n",
    "            ancestors = resampling(W[:,t-1], rs)\n",
    "            X[:,:t,:] = X[ancestors,:t,:]\n",
    "\n",
    "        # Propagation\n",
    "        X[:,t,:] = smc_obj.sim_prop(t, X[:,t-1,:], y, prop_params, model_params, rs)\n",
    "\n",
    "        # Weighting\n",
    "        logW = smc_obj.log_weights(t, X[:,t,:], X[:,t-1,:], y, prop_params, model_params)\n",
    "        max_logW = np.max(logW)\n",
    "        W[:,t] = np.exp(logW-max_logW)\n",
    "        W[:,t] /= np.sum(W[:,t])\n",
    "        ESS[t] = 1./np.sum(W[:,t]**2)\n",
    "\n",
    "    # Sample from the empirical approximation\n",
    "    bins = np.cumsum(W[:,-1])\n",
    "    u = rs.rand()\n",
    "    B = np.digitize(u,bins)\n",
    "\n",
    "    if verbose:\n",
    "        print('Mean ESS', np.mean(ESS)/N)\n",
    "        print('Min ESS', np.min(ESS))\n",
    "\n",
    "    return X[B,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy.random as npr\n",
    "\n",
    "def init_model_params(Dx, Dy, alpha, r, obs, rs = npr.RandomState(0)):\n",
    "    mu0    = np.zeros(Dx)\n",
    "    Sigma0 = np.eye(Dx)\n",
    "    A      = np.zeros((Dx,Dx))\n",
    "    for i in range(Dx):\n",
    "        for j in range(Dx):\n",
    "            A[i,j] = alpha**(abs(i-j)+1)\n",
    "\n",
    "    Q = np.eye(Dx)\n",
    "    C = np.zeros((Dy,Dx))\n",
    "    if obs == 'sparse':\n",
    "        C[:Dy,:Dy] = np.eye(Dy)\n",
    "    else:\n",
    "        C = rs.normal(size=(Dy,Dx))\n",
    "    R = r * np.eye(Dy)\n",
    "\n",
    "    return (mu0, Sigma0, A, Q, C, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_prop_params(T, Dx, scale = 0.5, rs = npr.RandomState(0)):\n",
    "    return [(scale * rs.randn(Dx), # Bias\n",
    "             1. + scale * rs.randn(Dx), # Linear times A/mu0\n",
    "             scale * rs.randn(Dx)) # Log-var\n",
    "            for t in range(T)]\n",
    "\n",
    "def generate_data(model_params, T = 5, rs = npr.RandomState(0)):\n",
    "    mu0, Sigma0, A, Q, C, R = model_params\n",
    "    Dx = mu0.shape[0]\n",
    "    Dy = R.shape[0]\n",
    "\n",
    "    x_true = np.zeros((T,Dx))\n",
    "    y_true = np.zeros((T,Dy))\n",
    "\n",
    "    for t in range(T):\n",
    "        if t > 0:\n",
    "            x_true[t,:] = rs.multivariate_normal(np.dot(A,x_true[t-1,:]),Q)\n",
    "        else:\n",
    "            x_true[0,:] = rs.multivariate_normal(mu0,Sigma0)\n",
    "        y_true[t,:] = rs.multivariate_normal(np.dot(C,x_true[t,:]),R)\n",
    "\n",
    "    return x_true, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_marginal_likelihood(model_params, T, y_true):\n",
    "    mu0, Sigma0, A, Q, C, R = model_params\n",
    "    Dx = mu0.shape[0]\n",
    "    Dy = R.shape[1]\n",
    "\n",
    "    log_likelihood = 0.\n",
    "    xfilt = np.zeros(Dx)\n",
    "    Pfilt = np.zeros((Dx,Dx))\n",
    "    xpred = mu0\n",
    "    Ppred = Sigma0\n",
    "\n",
    "    for t in range(T):\n",
    "        if t > 0:\n",
    "            # Predict\n",
    "            xpred = np.dot(A,xfilt)\n",
    "            Ppred = np.dot(A,np.dot(Pfilt,A.T)) + Q\n",
    "\n",
    "        # Update\n",
    "        yt = y_true[t,:] - np.dot(C,xpred)\n",
    "        S = np.dot(C,np.dot(Ppred,C.T)) + R\n",
    "        K = np.linalg.solve(S, np.dot(C,Ppred)).T\n",
    "        xfilt = xpred + np.dot(K,yt)\n",
    "        Pfilt = Ppred - np.dot(K,np.dot(C,Ppred))\n",
    "\n",
    "        sign, logdet = np.linalg.slogdet(S)\n",
    "        log_likelihood += -0.5*(np.sum(yt*np.linalg.solve(S,yt)) + logdet + Dy*np.log(2.*np.pi))\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lgss_smc:\n",
    "    \"\"\"\n",
    "    Class for defining functions used in variational SMC.\n",
    "    \"\"\"\n",
    "    def __init__(self, T, Dx, Dy, N):\n",
    "        self.T = T\n",
    "        self.Dx = Dx\n",
    "        self.Dy = Dy\n",
    "        self.N = N\n",
    "\n",
    "    def log_normal(self, x, mu, Sigma):\n",
    "        dim = Sigma.shape[0]\n",
    "        sign, logdet = np.linalg.slogdet(Sigma)\n",
    "        log_norm = -0.5*dim*np.log(2.*np.pi) - 0.5*logdet\n",
    "        Prec = np.linalg.inv(Sigma)\n",
    "        return log_norm - 0.5*np.sum((x-mu)*np.dot(Prec,(x-mu).T).T,axis=1)\n",
    "\n",
    "    def log_prop(self, t, Xc, Xp, y, prop_params, model_params):\n",
    "        mu0, Sigma0, A, Q, C, R = model_params\n",
    "        mut, lint, log_s2t = prop_params[t]\n",
    "        s2t = np.exp(log_s2t)\n",
    "\n",
    "        if t > 0:\n",
    "            mu = mut + np.dot(A, Xp.T).T*lint\n",
    "        else:\n",
    "            mu = mut + lint*mu0\n",
    "\n",
    "        return self.log_normal(Xc, mu, np.diag(s2t))\n",
    "\n",
    "    def log_target(self, t, Xc, Xp, y, prop_params, model_params):\n",
    "        mu0, Sigma0, A, Q, C, R = model_params\n",
    "        if t > 0:\n",
    "            logF = self.log_normal(Xc,np.dot(A,Xp.T).T, Q)\n",
    "        else:\n",
    "            logF = self.log_normal(Xc, mu0, Sigma0)\n",
    "        logG = self.log_normal(np.dot(C,Xc.T).T, y[t], R)\n",
    "        return logF + logG\n",
    "\n",
    "    # These following 2 are the only ones needed by variational-smc.py\n",
    "    def log_weights(self, t, Xc, Xp, y, prop_params, model_params):\n",
    "        return self.log_target(t, Xc, Xp, y, prop_params, model_params) - \\\n",
    "               self.log_prop(t, Xc, Xp, y, prop_params, model_params)\n",
    "\n",
    "    def sim_prop(self, t, Xp, y, prop_params, model_params, rs = npr.RandomState(0)):\n",
    "        mu0, Sigma0, A, Q, C, R = model_params\n",
    "        mut, lint, log_s2t = prop_params[t]\n",
    "        s2t = np.exp(log_s2t)\n",
    "\n",
    "        if t > 0:\n",
    "            mu = mut + np.dot(A, Xp.T).T*lint\n",
    "        else:\n",
    "            mu = mut + lint*mu0\n",
    "        return mu + rs.randn(*Xp.shape)*np.sqrt(s2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "True log-marginal likelihood: -46.89174359319463\n"
     ]
    }
   ],
   "source": [
    "# Model hyper-parameters\n",
    "T      = 10\n",
    "Dx     = 5\n",
    "Dy     = 3\n",
    "alpha  = 0.42\n",
    "r      = .1\n",
    "obs    = 'sparse'\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.5\n",
    "num_epochs = 1000\n",
    "step_size = 0.001\n",
    "\n",
    "N = 4\n",
    "\n",
    "data_seed = npr.RandomState(0)\n",
    "model_params = init_model_params(Dx, Dy, alpha, r, obs, data_seed)\n",
    "\n",
    "print(\"Generating data...\")\n",
    "x_true, y_true = generate_data(model_params, T, data_seed)\n",
    "\n",
    "lml = log_marginal_likelihood(model_params, T, y_true)\n",
    "print(\"True log-marginal likelihood: \"+str(lml))\n",
    "\n",
    "seed = npr.RandomState(0)\n",
    "\n",
    "# Initialize proposal parameters\n",
    "prop_params = init_prop_params(T, Dx, param_scale, seed)\n",
    "combined_init_params = (model_params, prop_params)\n",
    "\n",
    "lgss_smc_obj = lgss_smc(T, Dx, Dy, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training objective\n",
    "def objective(combined_params, iter):\n",
    "    model_params, prop_params = combined_params\n",
    "    return -vsmc_lower_bound(prop_params, model_params, y_true, lgss_smc_obj, seed)\n",
    "\n",
    "# Get gradients of objective using autograd.\n",
    "objective_grad = grad(objective)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0| -136.52003692786687\n",
      "            100|  -98.77446240248345\n",
      "            200|   -57.6303750970804\n",
      "            300|  -54.45751571342868\n",
      "            400|  -65.08079520031357\n",
      "            500|  -54.67798149962506\n",
      "            600| -43.741817571752065\n",
      "            700|  -50.18870619980928\n",
      "            800|  -55.92935048354515\n",
      "            900| -37.706093153539875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "\n",
    "def print_perf(combined_params, iter, grad):\n",
    "    if iter % (num_epochs/10) == 0:\n",
    "        model_params, prop_params = combined_params\n",
    "        bound = -objective(combined_params, iter)\n",
    "        message = \"{:15}|{:20}\".format(iter, bound)\n",
    "        print(message)\n",
    "        #with open(f_head+'_ELBO.csv', 'a') as f_handle:\n",
    "        #    np.savetxt(f_handle, [[iter,bound]], fmt='%i,%f')\n",
    "\n",
    "# SGD with adaptive step-size \"adam\"\n",
    "optimized_params = adam(objective_grad, combined_init_params, step_size=step_size,\n",
    "                        num_iters=num_epochs, callback=print_perf)\n",
    "opt_model_params, opt_prop_params = optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5,), (5, 5), (5, 5), (5, 5), (3, 5))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_model_params[0].shape, opt_model_params[1].shape, opt_model_params[2].shape, opt_model_params[3].shape, opt_model_params[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
